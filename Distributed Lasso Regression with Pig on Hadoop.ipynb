{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Lasso Regression with Apache Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*By Alexandre COMBESSIE and Thibaut DUGUET*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Project context and objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has been developed as part of the course \"Practical tools for the analysis of Big Data\" at ENSAE ParisTech, taught by Xavier Dupr√© and Mathieu Durut. It is one of the components of the Specialized Master in Data Science at ENSAE. The course website and material is available at <http://www.xavierdupre.fr/app/ensae_teaching_cs/helpsphinx3/td_3a.html>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set ourselves 3 key objectives for this project:\n",
    "- Understanding the principles of distributed computing on Apache Hadoop\n",
    "- Applying this knowledge to the case of Lasso Regression, by implementing a distributed algorithm in the programming language Pig Latin\n",
    "- Testing our algorithm on a real \"Big Data\" set to verify its functioning and derive useful insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Algorithm implemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our overall statistical framework is similar to the Ordinary Least Square (OLS) linear regression. We assume that we have a set of independent, identically distributed samples $\\{(x_1,y_1)...(x_n,y_n)\\}$ where $x_i \\in \\mathbb{R}^d$ is the feature vector and $y_i \\in \\mathbb{R}$ the  response. We are looking to \"learn\" weights $w_1...w_d$ to fit:\n",
    "\n",
    "\\begin{equation}\n",
    "y=Xw+\\epsilon \\quad \\textrm{where} \\quad y=\\begin{pmatrix} y_1\\\\ ...\\\\ y_n \\end{pmatrix} \\in \\mathbb{R}^n, \\quad X= \\begin{bmatrix} x_1^T \\\\ ... \\\\ x_n^T \\end{bmatrix} \\in \\mathbb{R}^{n \\times d}, \\quad w=\\begin{pmatrix} w_1\\\\ ...\\\\ w_d \\end{pmatrix} \\in \\mathbb{R}^d, \\quad \\epsilon=\\begin{pmatrix} \\epsilon_1\\\\ ...\\\\ \\epsilon_n \\end{pmatrix} \\in \\mathbb{R}^n \\quad \\textrm{error term}\n",
    "\\end{equation}\n",
    "\n",
    "In the case of the OLS linear regression, the solution of this equation is simply given by:\n",
    "\n",
    "\\begin{equation}\n",
    " \\min_{w \\in \\mathbb{R}^d} \\mathcal{L}_{OLS}(w) \\quad \\textrm{with} \\quad \\mathcal{L}_{OLS}(w)=\\tfrac{1}{2} \\left \\| Xw-y \\right \\|^2_2 \\quad \\textrm{loss function}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Lasso regression, the framework stays the same, but we use a slightly modified loss function in the minimization program, by adding a $L_1$-norm regularization term:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{L}_{lasso}(w)=\\tfrac{1}{2} \\left \\| Xw-y \\right \\|^2_2 + \\lambda \\left \\| w \\right \\|_1 \\quad \\textrm{with $\\lambda$ an external parameter}\n",
    "\\end{equation}\n",
    "\n",
    "The goal of this regularization term is to encourage the sparsity of the weight vector $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm for solzing  the minimization program of $\\mathcal{L}_{lasso}(w)$ is based on distributed coordinate descent. We start by splitting our data by feature:\n",
    " <img src=\"http://i.imgur.com/NtS41eM.png\">\n",
    " \n",
    "Then, for each feature, we perform a coordinate descent using the proximal gradient of $\\mathcal{L}_{lasso}(w)$ with respect to $w_i$. That is where our splitting strategy becomes useful, as is simplifies the gradient computation. Note that we use the concept of *proximal gradient* to overcome the problem of non-derivability of the $L_1$-norm. We will thus use the proximity operator defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\left(\\operatorname{prox}_{\\lambda}(x)\\right)_i = x_i-\\lambda \\quad \\textrm{if} \\quad x_i>\\lambda, \\quad 0\\quad \\textrm{if} \\quad|x_i|\\leq\\lambda, \\quad x_i+\\lambda\\quad \\textrm{if} \\quad x_i<-\\lambda\n",
    "\\end{equation}\n",
    "\n",
    "We will use this proximity operator on $\\nabla_j \\big(\\mathcal{L}_{OLS}(w) \\big)$, the gradient of $\\tfrac{1}{2}\\left \\| Xw-y \\right \\|^2_2$ with respect to $w_j$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_j (\\mathcal{L}_{OLS}(w))=\\frac{\\partial }{\\partial w_j} ( \\tfrac{1}{2}\\left \\| Xw-y \\right \\|^2_2 )=\\sum ^n_{i=1}{x_{ij}( \\sum^d_{k=1}{x_{ik}w_k-y_i})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize, our distributed algorithm can be described as:\n",
    "\n",
    "                Initialize w_1 ... w_d = 0\n",
    "                for t = 1 ... T iteration:\n",
    "                    for j = 1 ... d feature:\n",
    "                        g_j = GRADIENT_j(L_OLS(w))\n",
    "                        w_j^{t+1} = PROXIMITYOPERATOR(w_j^{t} - gamma * g_j)\n",
    "\n",
    "Our input is the observed matrix $X$ and the response vector $y$. Our output is the weight vector $w$. We have 2 parameters: $\\lambda$ for controling the thresholding effect of the proximity operator, and $\\gamma$ for controlling the gradient step. \n",
    "\n",
    "Thanks to mathematical properties of the proximal gradient, this algorithm should converge to the minimum of the loss function and solve the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm is mostly based on two academic papers:\n",
    "- Joseph K. Bradley, Aapo Kyrola, Danny Bickson, and Carlos Guestrin (2011). \n",
    "\"Parallel Coordinate Descent for L1-Regularized Loss Minimization.\" \n",
    "International Conference on Machine Learning (ICML 2011).\n",
    "http://arxiv.org/abs/1105.5379\n",
    "- Zeng, Jichuan, Haiqin Yang, Irwin King, and Michael R. Lyu (2014). \"A Comparison of Lasso-type Algorithms on Distributed Parallel Machine Learning Platforms.\" Neural Information Processing Systems Workshop (NIPS 2014). http://web.stanford.edu/~rezab/nips2014workshop/submits/plasso.pdf\n",
    "\n",
    "\n",
    "Other sources have been used and are listed here: https://github.com/alteralec/ENSAE_Big-Data_Project-Lasso/tree/master/Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Workspace & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyensae\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import plotly\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Connection to the Cloudera cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"background-color:gainsboro; padding:2px; border:0px;\"><b>server + credentials</b>\n",
       "<br />password <input type=\"password\" id=\"paramspassword\" value=\"\" size=\"80\" />\n",
       "<br />server <input type=\"text\" id=\"paramsserver\" value=\"\" size=\"80\" />\n",
       "<br />username <input type=\"text\" id=\"paramsusername\" value=\"\" size=\"80\" />\n",
       "<br /><button onclick=\"set_valueparams()\">Ok</button></div>\n",
       "<script type=\"text/Javascript\">\n",
       "function paramscallback(msg) {\n",
       "   var ret = msg.content.data['text/plain'];\n",
       "   $('#outparams').text(ret);\n",
       "}\n",
       "function set_valueparams(){\n",
       "   command='params = {' ;\n",
       "   var paramspasswordvar_value = document.getElementById('paramspassword').value;\n",
       "   command += '\"password\":\"' + paramspasswordvar_value + '\",';\n",
       "   var paramsservervar_value = document.getElementById('paramsserver').value;\n",
       "   command += '\"server\":\"' + paramsservervar_value + '\",';\n",
       "   var paramsusernamevar_value = document.getElementById('paramsusername').value;\n",
       "   command += '\"username\":\"' + paramsusernamevar_value + '\",';\n",
       "   command += '}';\n",
       "   var kernel = IPython.notebook.kernel;\n",
       "   kernel.execute(command);\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyquickhelper.ipythonhelper import open_html_form\n",
    "params={\"server\":\"\",\"username\":\"\",\"password\":\"\"}\n",
    "open_html_form(params=params,title=\"server + credentials\", key_save=\"params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "password = params[\"password\"]\n",
    "server = params[\"server\"]\n",
    "username = params[\"username\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyensae.remote.ssh_remote_connection.ASSHClient at 0xb1f7b38>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssh = %remote_open\n",
    "ssh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the SSH connection to the cluster is open, we can upload our datasets:\n",
    "1. A small dataset of simulated random variables with known relations\n",
    "2. A larger dataset of \"real\" variables, a sample of the popular Airline Dataset (http://stat-computing.org/dataexpo/2009/the-data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%remote_up SimulatedData.csv simulatedData.csv\n",
    "%remote_up weights.csv weights.csv\n",
    "%remote_up airline_sample5.csv airline_sample5.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%remote_cmd hdfs dfs -put simulatedData.csv ./simulatedData.csv\n",
    "%remote_cmd hdfs dfs -put weights.csv ./weights.csv\n",
    "%remote_cmd hdfs dfs -put airline_sample5.csv  ./airline_sample5.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that our data has been correctly stored on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "Found 1 items\n",
       "-rw-r--r--   3 acombessie acombessie  130015739 2016-01-31 20:50 airline_sample5.csv\n",
       "Found 1 items\n",
       "-rw-r--r--   3 acombessie acombessie     128829 2016-01-26 20:40 simulatedData.csv\n",
       "Found 1 items\n",
       "-rw-r--r--   3 acombessie acombessie         19 2016-01-26 20:40 weights.csv\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%remote_cmd hdfs dfs -ls *.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This algorithm was developed based on the simulated dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%PIG count_observations.pig\n",
    "A = LOAD '$input'\n",
    "        USING PigStorage(';')\n",
    "        AS (y:double, f1:double, f2:double, f3:double, f4:double, f5:double, f6:double, f7:double, f8:double, \n",
    "            f9:double, f10:double);\n",
    "B = GROUP A ALL;\n",
    "C = FOREACH B GENERATE COUNT(A);\n",
    "STORE C\n",
    "INTO '$output'\n",
    "USING PigStorage(';') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', '')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssh.pig_submit(\"count_observations.pig\", redirection=\"redirection\",\n",
    "               params = dict(input='simulatedData.csv', output='count_simulatedData'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "Total bytes written : 5\n",
       "Spillable Memory Manager spill count : 0\n",
       "Total bags proactively spilled: 0\n",
       "Total records proactively spilled: 0\n",
       "\n",
       "Job DAG:\n",
       "job_1453928987583_0334\n",
       "\n",
       "\n",
       "2016-01-30 16:56:08,438 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%remote_cmd tail redirection.err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to verify that our script worked, we download the result from the cluster and store it as a local variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000. It works!\n"
     ]
    }
   ],
   "source": [
    "%remote_cmd hdfs dfs -getmerge count_simulatedData count_simulatedData\n",
    "if os.path.exists(\"count_simulatedData\"): os.remove(\"count_simulatedData\")\n",
    "%remote_down count_simulatedData count_simulatedData\n",
    "nb_obs=int(np.loadtxt(\"count_simulatedData\",delimiter=\";\"))\n",
    "print(str(nb_obs)+\". It works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need a script to standardize our datasets in terms of mean and standard error, while storing these values for each columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%PIG standardize.pig\n",
    "A = LOAD '$input'\n",
    "        USING PigStorage(';')\n",
    "        AS (y:double, f1:double, f2:double, f3:double, f4:double, f5:double, f6:double, f7:double, f8:double, \n",
    "            f9:double, f10:double);\n",
    "B = GROUP A ALL;\n",
    "C = FOREACH B\n",
    "    GENERATE (double) AVG(A.y) AS ym, (double) AVG(A.f1) AS f1m, (double) AVG(A.f2) AS f2m, (double) AVG(A.f3) AS f3m, \n",
    "    (double) AVG(A.f4) AS f4m, (double) AVG(A.f5) AS f5m, (double) AVG(A.f6) AS f6m, (double) AVG(A.f7) AS f7m, \n",
    "    (double) AVG(A.f8) AS f8m, (double) AVG(A.f9) AS f9m, (double) AVG(A.f10) AS f10m;\n",
    "D = FOREACH A\n",
    "    GENERATE y - (double)C.ym AS y, f1 - (double)C.f1m AS f1, f2 - (double)C.f2m AS f2, f3 - (double)C.f3m AS f3, \n",
    "    f4 - (double)C.f4m AS f4, f5 - (double)C.f5m AS f5, f6 - (double)C.f6m AS f6, f7 - (double)C.f7m AS f7, \n",
    "    f8 - (double)C.f8m AS f8, f9 - (double)C.f9m AS f9, f10 - (double)C.f10m AS f10;\n",
    "D2 = FOREACH D\n",
    "     GENERATE (double) y*y AS y, (double) f1*f1 AS f1, (double) f2*f2 AS f2, (double) f3*f3 AS f3, (double) f4*f4 AS f4, \n",
    "    (double) f5*f5 AS f5, (double) f6*f6 AS f6, (double) f7*f7 AS f7,\n",
    "    (double) f8*f8 AS f8, (double) f9*f9 AS f9, (double) f10*f10 AS f10;\n",
    "E = GROUP D2 ALL;\n",
    "F = FOREACH E\n",
    "    GENERATE (double) AVG(D2.y) AS yv, (double) AVG(D2.f1) AS f1v, (double) AVG(D2.f2) AS f2v, (double) AVG(D2.f3) AS f3v, \n",
    "    (double) AVG(D2.f4) AS f4v, (double) AVG(D2.f5) AS f5v, (double) AVG(D2.f6) AS f6v, (double) AVG(D2.f7) AS f7v, \n",
    "    (double) AVG(D2.f8) AS f8v, (double) AVG(D2.f9) AS f9v, (double) AVG(D2.f10) AS f10v;\n",
    "G = FOREACH F\n",
    "    GENERATE (double) SQRT(F.yv) AS ye, (double) SQRT(F.f1v) AS f1e, (double) SQRT(F.f2v) AS f2e, (double) SQRT(F.f3v) AS f3e, \n",
    "    (double) SQRT(F.f4v) AS f4e, (double) SQRT(F.f5v) AS f5e, (double) SQRT(F.f6v) AS f6e, (double) SQRT(F.f7v) AS f7e, \n",
    "    (double) SQRT(F.f8v) AS f8e, (double) SQRT(F.f9v) AS f9e, (double) SQRT(F.f10v) AS f10e;\n",
    "H = FOREACH D\n",
    "    GENERATE y / (double)G.ye AS y, f1 / (double)G.f1e AS f1, f2 / (double)G.f2e AS f2, f3 / (double)G.f3e AS f3, \n",
    "    f4 / (double)G.f4e AS f4, f5 / (double)G.f5e AS f5, f6 / (double)G.f6e AS f6, f7 / (double)G.f7e AS f7, \n",
    "    f8 / (double)G.f8e AS f8, f9 / (double)G.f9e AS f9, f10 / (double)G.f10e AS f10;\n",
    "mean = FOREACH C\n",
    "        GENERATE (double) ym, (double) f1m, (double) f2m, (double) f3m, (double) f4m, (double) f5m, (double) f6m, \n",
    "        (double) f7m, (double) f8m, (double) f9m, (double) f10m;\n",
    "standarderror = FOREACH G\n",
    "                 GENERATE (double) ye, (double) f1e, (double) f2e, (double) f3e, (double) f4e, (double) f5e, (double) f6e, \n",
    "                (double) f7e, (double) f8e, (double) f9e, (double) f10e ;\n",
    "STORE H INTO '$output_dataset' USING PigStorage(';') ;\n",
    "STORE mean INTO '$output_mean' USING PigStorage(';') ;\n",
    "STORE standarderror INTO '$output_se' USING PigStorage(';') ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', '')"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssh.pig_submit(\"standardize.pig\", redirection=\"redirection\",\n",
    "               params = dict(input='simulatedData.csv',\n",
    "                             output_dataset='simulatedData_standard',\n",
    "                            output_mean='mean_simulatedData',\n",
    "                            output_se='se_simulatedData'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.22782665  2.39314643  2.45934351  2.51525212  2.48939266  2.46122243\n",
      "   2.55422532  2.52448601  2.46756322  2.5558095   2.54373562]\n",
      " [ 4.90413158  1.72037209  1.68864271  1.71590714  1.73155267  1.68990918\n",
      "   1.71688259  1.70245844  1.70297213  1.72821554  1.69839152]]\n"
     ]
    }
   ],
   "source": [
    "%remote_cmd hdfs dfs -getmerge mean_simulatedData mean_simulatedData\n",
    "%remote_cmd hdfs dfs -getmerge se_simulatedData se_simulatedData\n",
    "if os.path.exists(\"mean_simulatedData\"): os.remove(\"mean_simulatedData\")\n",
    "if os.path.exists(\"se_simulatedData\"): os.remove(\"se_simulatedData\")\n",
    "%remote_down mean_simulatedData mean_simulatedData\n",
    "%remote_down se_simulatedData se_simulatedData\n",
    "mean_sim=np.loadtxt(\"mean_simulatedData\",delimiter=\";\")\n",
    "se_sim=np.loadtxt(\"se_simulatedData\",delimiter=\";\")\n",
    "print(np.vstack((mean_sim,se_sim)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our script worked! We can now implement our algorithm on this standardized dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Coordinate Descent iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%PIG iteration.pig\n",
    "\n",
    "%declare gamma $gam\n",
    "%declare lambda $lam\n",
    "%declare nb_observation $count\n",
    "\n",
    "------------------------------------------------DATA LOADING\n",
    "INPUTFILE = LOAD '$normalizedinput'\n",
    "        USING PigStorage(';')\n",
    "        AS (y:double, f1:double, f2:double, f3:double, f4:double, f5:double, f6:double, f7:double, f8:double, \n",
    "            f9:double, f10:double);\n",
    "A = RANK INPUTFILE;\n",
    "W = LOAD '$previousWeights'\n",
    "        USING PigStorage(';')\n",
    "        AS (w1:double, w2:double, w3:double, w4:double, w5:double, w6:double, w7:double, w8:double, \n",
    "            w9:double, w10:double);\n",
    "\n",
    "        \n",
    "------------------------------------------------DATA PREPARATION        \n",
    "B = CROSS A, W;\n",
    "D = FOREACH B GENERATE $0 as id, \n",
    "                      y as y,\n",
    "                      (f1,f2,f3,f4,f5,f6,f7,f8,f9,f10) as vector,\n",
    "                      FLATTEN({(f1,w1,1),(f2,w2,2),(f3,w3,3),(f4,w4,4),(f5,w5,5),(f6,w6,6),(f7,w7,7),\n",
    "                               (f8,w8,8),(f9,w9,9),(f10,w10,10)});\n",
    "E = FOREACH D GENERATE id as id, \n",
    "                      y as y,\n",
    "                      vector as vector,\n",
    "                      TOTUPLE($3,$4,$5) as product;\n",
    "            \n",
    "------------------------------------------------PROXIMAL GRADIENT COMPUTATION            \n",
    "F = FOREACH E GENERATE id as id, \n",
    "                      y as y,\n",
    "                      vector as vector,\n",
    "                      product as product,\n",
    "                      product.$0 * product.$1 as prod; \n",
    "G = GROUP F by (id, y, vector);\n",
    "H = FOREACH G GENERATE group.id as id, \n",
    "                       group.y as y,\n",
    "                       group.vector as vector,\n",
    "                       SUM(F.prod) - group.y as diff;                \n",
    "I = FOREACH H GENERATE id as id,\n",
    "                       y as y,\n",
    "                       diff as diff,\n",
    "                       FLATTEN({(vector.$0,1),(vector.$1,2),(vector.$2,3),(vector.$3,4),(vector.$4,5),\n",
    "                               (vector.$5,6),(vector.$6,7),(vector.$7,8),(vector.$8,9),(vector.$9,10)});\n",
    "J = FOREACH I GENERATE id as id,\n",
    "                       y as y,\n",
    "                       diff * $3 as gradient,\n",
    "                       $4 as dim;                        \n",
    "K = GROUP J by dim;\n",
    "L = FOREACH K GENERATE group as dim,\n",
    "                       SUM(J.gradient)/$nb_observation as gradient;\n",
    "W2 = FOREACH W GENERATE FLATTEN({(w1,1),(w2,2),(w3,3),(w4,4),(w1,5),(w1,6),(w1,7),(w1,8),(w1,9),(w1,10)});\n",
    "M = JOIN L BY dim, W2 BY $1;\n",
    "N = FOREACH M GENERATE $0 as dim,\n",
    "                      (CASE\n",
    "                        WHEN $2 - $gamma * $1 > $lambda THEN $2 - $gamma * $1 - $lambda\n",
    "                        WHEN ABS($2 - $gamma * $1) <= $lambda THEN 0\n",
    "                        WHEN $2 - $gamma * $1 < - $lambda THEN $2 - $gamma * $1 + $lambda\n",
    "                       END) as w;\n",
    "    \n",
    "------------------------------------------------RETURN RESULTS\n",
    "N2 = GROUP N ALL;\n",
    "N3 = FOREACH N2 GENERATE FLATTEN(BagToTuple(N.w));\n",
    "O = JOIN N BY dim, W2 BY $1;\n",
    "P = FOREACH O GENERATE ABS($2 - $1) AS conv;\n",
    "Q = GROUP P ALL;\n",
    "R = FOREACH Q GENERATE SUM(P.conv) AS conv;\n",
    "STORE N3 INTO '$newWeights' USING PigStorage(';') ;\n",
    "STORE R INTO '$conv' USING PigStorage(';') ;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each iteration, we obtain the new weights, along with an indicator of convergence. Once we reach a convergence criterion, we can obtain the \"true\" weights for non-standardized data with the formula:\n",
    "\n",
    "\\begin{equation*}\n",
    "Weight_{true}=\\frac{Weight_{standard} \\times SE(response)}{SE(weight)} \\quad \\textrm{where} \\quad \\textrm{SE is the Standard Error}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can submit this script to verify that it works for 1 iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', '')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssh.pig_submit(\"iteration.pig\", redirection=\"redirection\", \n",
    "               params = dict(count=str(nb_obs),\n",
    "                             gam=str(0.1),\n",
    "                             lam=str(0.01),\n",
    "                             normalizedinput='simulatedData_standard/part-m-00000',\n",
    "                            previousWeights='weights.csv', \n",
    "                            newWeights='sim_weights1', \n",
    "                            conv='sim_conv1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "Found 2 items\n",
       "-rw-r--r--   3 acombessie acombessie          0 2016-01-30 18:30 sim_weights1/_SUCCESS\n",
       "-rw-r--r--   3 acombessie acombessie        159 2016-01-30 18:30 sim_weights1/part-r-00000\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%remote_cmd hdfs dfs -ls sim_weights1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>\n",
       "0.0;0.014271898565226605;-0.029926762944119813;0.0;0.06568621891779762;-0.012551785788727296;-0.02535137224581839;0.0;0.009142137071876876;0.00279567874350373\n",
       "\n",
       "</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%remote_cmd hdfs dfs -tail sim_weights1/part-r-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### c. Automating the iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement a Python script to launch a series of iteration on our dataset. For that, we will need a simple function to verify is the iteration is finished, and another function to retrieve the results from it (weights and convergence value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iteration_finished(output):\n",
    "    result = %remote_cmd hdfs dfs -ls $output\n",
    "    return(re.search(\"SUCCESS\", result.data) != None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getvalue_fromHDFS(x):\n",
    "    %remote_cmd hdfs dfs -getmerge $x $x\n",
    "    if os.path.exists(x): os.remove(x)\n",
    "    %remote_down $x $x\n",
    "    return(np.loadtxt(x,delimiter=\";\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a Python script to automate the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "max_iteration=30\n",
    "previousWeights_name = 'weights.csv'\n",
    "convergence=[]\n",
    "weight_matrix=np.array(range(1,11))\n",
    "while(i <= max_iteration):\n",
    "    newWeights_name=\"sim_weights\"+str(i)\n",
    "    conv_name=\"sim_conv\"+str(i)\n",
    "    state_job=False\n",
    "    ssh.pig_submit(\"iteration.pig\", redirection=\"redirection\", \n",
    "               params = dict(count=str(nb_obs),\n",
    "                            gam=str(0.1),\n",
    "                             lam=str(0.01),\n",
    "                             normalizedinput='simulatedData_standard/part-m-00000',\n",
    "                            previousWeights=previousWeights_name, \n",
    "                            newWeights=newWeights_name, \n",
    "                            conv=conv_name))\n",
    "    while(state_job==False):\n",
    "        state_job=iteration_finished(conv_name)                    \n",
    "        time.sleep(7)\n",
    "    else:  \n",
    "        new_weights=getvalue_fromHDFS(newWeights_name)\n",
    "        new_convergence=getvalue_fromHDFS(conv_name)\n",
    "        convergence=np.append(convergence,new_convergence)\n",
    "        weight_matrix=np.vstack((weight_matrix,new_weights))\n",
    "        print(str(i)+\": \"+str(new_weights.round(3))+\" convergence: \"+str(new_convergence.round(6)))\n",
    "        previousWeights_name = newWeights_name\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here below that our algorithm converges rapidly, which is not surprising since our test dataset is composed of random variables with a known link between the features and the response vector. We notice that some weights are always set to 0, which corresponds to the sparsity property of the the Lasso $L_1$-norm regularization. The $\\lambda$ parameter of the proximity operator also accelerates greatly the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~alteralec/4.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterate_range=np.arange(0,51)\n",
    "data=[]\n",
    "for i in range(0,10):\n",
    "    data.append(go.Scatter(x=iterate_range,y=np.append(0,weight_matrix[1:15,i]*se_sim[0]/se_sim[i+1]),\n",
    "                           mode='lines+markers',name=\"Weight \"+str(i+1)))\n",
    "py.iplot(data, filename='simulated_weightconvergence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~alteralec/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot([go.Bar(x=iterate_range+1,y=convergence[0:15])], filename='simulated_convergence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test of our algorithm on 'real' data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to test our algorithm on the chosen dataset, i.e. a sample of the popular \"Airline Dataset\" on airline on-time performance. It is available on http://stat-computing.org/dataexpo/2009/the-data.html). It covers flight arrival and departure details for all commercial flights in the USA from 1987 to 2008. \n",
    "\n",
    "We prepared this dataset beforehand, according to the recommendations of Michael Kane and Jay Emerson on http://stat-computing.org/dataexpo/2009/posters/kane-emerson.pdf. Following this data preparation step, we went from 120 to 72 million records. Because of computational issues on the cluster, we had to take only a sample of 5% of the prepared database, covering 3.6 million records for a size of 127MB.\n",
    "\n",
    "In this dataset, we chose to study the following 11 variables.\n",
    "- **Response**: delay upon arrival\n",
    "- **Features**:\n",
    "    - Year of the flight\n",
    "    - Month of the flight\n",
    "    - Age of the plane (estimated)\n",
    "    - Air Time\n",
    "    - Delay at departure\n",
    "    - Taxi out time at departure\n",
    "    - Distance\n",
    "    - Elapsed flight time\n",
    "    - Day of Month\n",
    "    - Day of Week\n",
    "\n",
    "Thanks to our Lasso algorithm, we can hope to determine what are the key factors explaining the flight delay upon arrival. Let's apply our scripts!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Counting observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', '')"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssh.pig_submit(\"count_observations.pig\", redirection=\"redirection\",\n",
    "               params = dict(input='airline_sample5.csv', output='count_airline'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3613346"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nobs_airline=getvalue_fromHDFS('count_airline')\n",
    "int(nobs_airline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('', '')"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ssh.pig_submit(\"standardize.pig\", redirection=\"redirection\",\n",
    "               params = dict(input='airline_sample5.csv',\n",
    "                             output_dataset='airline_standard',\n",
    "                            output_mean='mean_airline',\n",
    "                            output_se='se_airline'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means_airline=getvalue_fromHDFS('mean_airline')\n",
    "standarderrors_airline=getvalue_fromHDFS('se_airline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Age</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Distance</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>DayofMonth</th>\n",
       "      <th>DayOfWeek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean</th>\n",
       "      <td>7.82</td>\n",
       "      <td>2001.96</td>\n",
       "      <td>6.56</td>\n",
       "      <td>30.83</td>\n",
       "      <td>103.60</td>\n",
       "      <td>8.94</td>\n",
       "      <td>15.67</td>\n",
       "      <td>737.04</td>\n",
       "      <td>125.38</td>\n",
       "      <td>15.74</td>\n",
       "      <td>3.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Standard error</th>\n",
       "      <td>33.71</td>\n",
       "      <td>4.29</td>\n",
       "      <td>3.40</td>\n",
       "      <td>64.10</td>\n",
       "      <td>71.85</td>\n",
       "      <td>31.31</td>\n",
       "      <td>10.96</td>\n",
       "      <td>564.36</td>\n",
       "      <td>70.18</td>\n",
       "      <td>8.79</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                ArrDelay     Year  Month    Age  AirTime  DepDelay  TaxiOut  \\\n",
       "Mean                7.82  2001.96   6.56  30.83   103.60      8.94    15.67   \n",
       "Standard error     33.71     4.29   3.40  64.10    71.85     31.31    10.96   \n",
       "\n",
       "                Distance  ActualElapsedTime  DayofMonth  DayOfWeek  \n",
       "Mean              737.04             125.38       15.74       3.95  \n",
       "Standard error    564.36              70.18        8.79       1.99  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(np.vstack((means_airline,standarderrors_airline)).round(2),columns=column_names,index=[\"Mean\",\"Standard error\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lasso algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "max_iteration=20\n",
    "previousWeights_name = 'weights.csv'\n",
    "convergence_airline=[]\n",
    "weight_matrix_airline=np.array(range(1,11))\n",
    "while(i <= max_iteration):\n",
    "    newWeights_name=\"airline_weights\"+str(i)\n",
    "    conv_name=\"airline_conv\"+str(i) \n",
    "    state_job=False\n",
    "    ssh.pig_submit(\"iteration.pig\", redirection=\"redirection\", \n",
    "               params = dict(count=str(nobs_airline),\n",
    "                            gam=str(0.1),\n",
    "                             lam=str(0.001),\n",
    "                             normalizedinput='airline_standard',\n",
    "                            previousWeights=previousWeights_name, \n",
    "                            newWeights=newWeights_name, \n",
    "                            conv=conv_name))\n",
    "    while(state_job==False):\n",
    "        state_job=iteration_finished(conv_name)                    \n",
    "        time.sleep(7)\n",
    "    else:  \n",
    "        new_weights_airline=getvalue_fromHDFS(newWeights_name)\n",
    "        new_convergence_airline=getvalue_fromHDFS(conv_name)\n",
    "        convergence_airline=np.append(convergence_airline,new_convergence_airline)\n",
    "        weight_matrix_airline=np.vstack((weight_matrix_airline,new_weights_airline))\n",
    "        print(str(i)+\": \"+str(new_weights_airline.round(3))+\" convergence: \"+str(new_convergence_airline.round(6)))\n",
    "        previousWeights_name = newWeights_name\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~alteralec/6.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterate_range=np.arange(0,51)\n",
    "column_names=[\"ArrDelay\",\"Year\",\"Month\",\"Age\",\"AirTime\",\"DepDelay\",\"TaxiOut\",\n",
    "            \"Distance\",\"ActualElapsedTime\",\"DayofMonth\",\"DayOfWeek\"]\n",
    "data=[]\n",
    "for i in range(0,10):\n",
    "    data.append(go.Scatter(x=iterate_range,\n",
    "                           y=np.append(0,weight_matrix_airline[1:15,i]*standarderrors_airline[0]/standarderrors_airline[i+1]),\n",
    "                           mode='lines+markers',name=column_names[i+1]))\n",
    "py.iplot(data, filename='airline_weightconvergence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm converges rapidly in terms of number of iteration. However, it is quite computationally intensive: one iteration takes between 30 and 40 minutes to give its results. Hence, we had to run our algorithm for several hours.\n",
    "\n",
    "Because of the sparsity property of the Lasso, many weights as set to 0, which indicates a possible lack of correlation between the delay upon arrival (*our response*) and the feature. This stays true even when we reduce the $\\lambda$ proximity parameter, which is 0.001 here instead of 0.01 previously.\n",
    "\n",
    "It would mean that the 2 key factors explaining the delay upon arrival are:\n",
    "1. Delay at departure\n",
    "2. Taxi out time at departure\n",
    "\n",
    "While the first factor is obvious (direct causation), the second factor is less so. It would seem Taxi out time explains a lot of delay at arrival. It is defined as the \"period of time between when an airplane leaves the gate and when it takes off\". It means that the primary factors of delay do not come during the flight, but before the plane takes off. It is no so surprising, but still an interesting insight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OLS regression algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final test of our algorithm, we would like to see if it could also be used to fit an Ordinary Least Square regression model to our dataset. This model is actually a special case of our algorithm, with the proximity paramater $\\lambda=0$. \n",
    "\n",
    "For this experiment, we set the step size $\\gamma=1$ for the first 20 iterations, then $\\gamma=0.5$ in order to ensure the convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "max_iteration=20\n",
    "previousWeights_name = 'weights.csv'\n",
    "convergence_airline_ols=[]\n",
    "weight_matrix_airline_ols=np.array(range(1,11))\n",
    "while(i <= max_iteration):\n",
    "    newWeights_name=\"airline_weights_ols\"+str(i)\n",
    "    conv_name=\"airline_conv_ols\"+str(i) \n",
    "    state_job=False\n",
    "    ssh.pig_submit(\"iteration.pig\", redirection=\"redirection\", \n",
    "               params = dict(count=str(nobs_airline),\n",
    "                            gam=str(1.0),\n",
    "                             lam=str(0.0),\n",
    "                             normalizedinput='airline_standard',\n",
    "                            previousWeights=previousWeights_name, \n",
    "                            newWeights=newWeights_name, \n",
    "                            conv=conv_name))\n",
    "    while(state_job==False):\n",
    "        state_job=iteration_finished(conv_name)                    \n",
    "        time.sleep(7)\n",
    "    else:  \n",
    "        new_weights_airline_ols=getvalue_fromHDFS(newWeights_name)\n",
    "        new_convergence_airline_ols=getvalue_fromHDFS(conv_name)\n",
    "        convergence_airline_ols=np.append(convergence_airline_ols,new_convergence_airline_ols)\n",
    "        weight_matrix_airline_ols=np.vstack((weight_matrix_airline_ols,new_weights_airline_ols))\n",
    "        print(str(i)+\": \"+str(new_weights_airline_ols.round(3))+\" convergence: \"+str(new_convergence_airline_ols.round(6)))\n",
    "        previousWeights_name = newWeights_name\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i=21\n",
    "max_iteration=40\n",
    "while(i <= max_iteration):\n",
    "    newWeights_name=\"airline_weights_ols\"+str(i)\n",
    "    conv_name=\"airline_conv_ols\"+str(i) \n",
    "    state_job=False\n",
    "    ssh.pig_submit(\"iteration.pig\", redirection=\"redirection\", \n",
    "               params = dict(count=str(nobs_airline),\n",
    "                            gam=str(0.5),\n",
    "                             lam=str(0.0),\n",
    "                             normalizedinput='airline_standard',\n",
    "                            previousWeights=previousWeights_name, \n",
    "                            newWeights=newWeights_name, \n",
    "                            conv=conv_name))\n",
    "    while(state_job==False):\n",
    "        state_job=iteration_finished(conv_name)                    \n",
    "        time.sleep(7)\n",
    "    else:  \n",
    "        new_weights_airline_ols=getvalue_fromHDFS(newWeights_name)\n",
    "        new_convergence_airline_ols=getvalue_fromHDFS(conv_name)\n",
    "        convergence_airline_ols=np.append(convergence_airline_ols,new_convergence_airline_ols)\n",
    "        weight_matrix_airline_ols=np.vstack((weight_matrix_airline_ols,new_weights_airline_ols))\n",
    "        print(str(i)+\": \"+str(new_weights_airline_ols.round(3))+\" convergence: \"+str(new_convergence_airline_ols.round(6)))\n",
    "        previousWeights_name = newWeights_name\n",
    "        i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_matrix_airline_ols=np.array(range(1,11))\n",
    "convergence_airline_ols=[]\n",
    "for i in range(1,41):\n",
    "    newWeights_name=\"airline_weights_ols\"+str(i)\n",
    "    conv_name=\"airline_conv_ols\"+str(i)\n",
    "    new_weights_airline_ols=getvalue_fromHDFS(newWeights_name)\n",
    "    new_convergence_airline_ols=getvalue_fromHDFS(conv_name)\n",
    "    weight_matrix_airline_ols=np.vstack((weight_matrix_airline_ols,new_weights_airline_ols))\n",
    "    convergence_airline_ols=np.append(convergence_airline_ols,new_convergence_airline_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~alteralec/8.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iterate_range=np.arange(0,51)\n",
    "column_names=[\"ArrDelay\",\"Year\",\"Month\",\"Age\",\"AirTime\",\"DepDelay\",\"TaxiOut\",\n",
    "            \"Distance\",\"ActualElapsedTime\",\"DayofMonth\",\"DayOfWeek\"]\n",
    "data=[]\n",
    "for i in range(0,10):\n",
    "    data.append(go.Scatter(x=iterate_range,\n",
    "                        y=np.append(0,weight_matrix_airline_ols[1:42,i]*standarderrors_airline[0]/standarderrors_airline[i+1]),\n",
    "                        mode='lines+markers',name=column_names[i+1]))\n",
    "py.iplot(data, filename='airline_weightconvergence_ols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\"seamless=\"seamless\" src=\"https://plot.ly/~alteralec/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot([go.Bar(x=iterate_range+1,y=convergence_airline_ols[0:41])], filename='airline_convergence_ols')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our correction of the step size parameter at the 21st iteration, we see that our algorithm converges correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All in all, we managed to successfully overcome several challenges, but some remain:\n",
    "\n",
    "- **Successes**\n",
    "    - Distribute a simple Machine Learning algorithm on Hadoop.\n",
    "    - Learn the programming language Pig Latin.\n",
    "    - Manage jobs on a Hadoop cluster. For that we were glad to choose a Cloudera setup, which has the very useful `%job_syntax` command, the ability to check the log `redirection.err`, and the great Hue interface\n",
    "    - Explore an interesting new dataset\n",
    "    \n",
    "    \n",
    "- **To be improved**\n",
    "    - Make our algorithm more scalable, as we run into problems when trying with large datasets. We believe the problem lies with our data splitting strategy by column, which obliges us to transform the dataset with Pig (costly and inefficient in terms of computation). A possible solution would be to transform the text file beforehand using Python, with a line-by-line processing to limit memory usage.\n",
    "    - Optimize the choice of our parameters $\\lambda,\\gamma$. We could possibly do that using cross-validation, or use dynamic updating during iterations.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%remote_close"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
